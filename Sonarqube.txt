Sonar qube

link:
https://www.crestdatasys.com/blogs/an-introduction-on-using-sonarqube/#:~:text=SonarQube%20is%20a%20Code%20Quality,be%20measured%20continually%20over%20time.

.SonarQube is a Code Quality Assurance tool that collects and analyzes source code, and provides reports for the code quality of your project.
.It combines static and dynamic analysis tools and enables quality to be measured continually over time. Everything from minor styling choices, to design errors are inspected and evaluated by SonarQube. This provides users with a rich searchable history of the code to analyze where the code is messing up and determine whether or not it is styling issues, code defeats, code duplication, lack of test coverage, or excessively complex code.
.The software will analyze source code from different aspects and drills down the code layer by layer, moving module level down to the class level, with each level producing metric values and statistics that should reveal problematic areas in the source code that needs improvement.
****************************************
Quality profile and Quality gate:
Every project has a quality profile set for each supported language. The profile defines which rules will be applied during analysis. After analysis, the quality gate takes the resulting metrics and compares them to its defined thresholds to determine if the code meets the requirements for release or merge

Quality Profiles are a core component of SonarQube, since they are where you define sets of Rules that when violated should raise issues on your codebase (example: Methods should not have a Cognitive Complexity higher than 15). Quality Profiles are defined for individual languages.

Quality Gates are the set of conditions a project must meet before it should be pushed to further environments. Quality Gates considers all of the quality metrics for a project and assigns a passed or failed designation for that project.
What are the different types of quality gates?
Gates of Quality
1) Creating a Quality Strategy.
2) Concise User Stories and Acceptance.
3) Creating Test Scenarios.
4) Pair Testing with Developers.
5) Performing Manual Verification.
6) Automated Regression Test.

A Quality Gate outputs a status (Pass, Warn, Fail)

Go to Manage Jenkins > Configure System and scroll down to the SonarQube servers section

*************************************************

plugins:
Plugins are the primary means of enhancing the functionality of a Jenkins environment to suit organization- or user-specific needs. 
A plug-in is a piece of software that adds new features or extends functionality on an existing application.
**************************************

Rolling update: Rolling release, also known as rolling update or continuous delivery, is a concept in software development of frequently delivering updates to applications. 
The rolling update strategy is a gradual process that allows you to update your k8s s/y with only a minor effect on performance and no downtime.
In this strategy, The deployments select a pod with an old programming, deactivate it, and create an updated pod to replace it. The deployments repeat this process until no outdated pods remain.

the advantage is the update is applied pod by pod so the greater s/y can remain active.

libk:https://medium.com/codex/kubernetes-deployment-rolling-updates-and-rollbacks-explained-e3efa6557368

The Rolling update uses two important parameters. They are listed below.

1. maxSurge

2. maxUnavailable

1. maxSurge => Here we assigned 25%. This ensures at most 125% of the desired number of Pods are up and running. 
Assume we have four Pods. So the 25% is equal to one Pod. This ensures 5 Pods are up and running if the traffic increases.
only a certain  number of pods created above the desired number of pods

2. maxUnavailable => Here we assigned 25%. This ensures at most 75% of the desired number of Pods are up and running. 
Assume we have four Pods. So the 25% is equal to one Pod. This ensures a minimum of three Pods are up and running always.
only certain number of pods are  unavailable during an update

both values can be desired no or percenrtage of desired pods

ex:max number of pods available: replica + max surge 4+1 =5
min: replica - max unavailable 4-1=3
i.e minimum 3 pods will be running during an update.

*************************************************************************
Rollout: The ability to deploy lates minor version (bugfix, hotfix, minor feature, enhancement) without downtime. Rollback: The ability to restore back the older working version in case something goes wrong.

***************************************************************************
Recreate: aLl the pods are killed all at once and replaced all at once with the new ones
********************************************
Edge location:Edge Location is the Data Center used to deliver content fast to your users. It is the site that is nearest your users.
CloudFront delivers your content through a worldwide network of data centers called edge locations. When a user requests content that you're serving with CloudFront, the request is routed to the edge location that provides the lowest latency (time delay), so that content is delivered with the best possible performance.


What is the difference between CDN and edge location?
The content delivery network has been around for a long time and is really just an early version of edge computing. The main difference is that a CDN focuses on transmitting cached data, while edge computing supports many other types of computing like live streaming, gaming, and AI
***********************************************************

A Docker registry is a system for versioning, storing and distributing Docker images. DockerHub is a hosted registry used by default when installing the Docker engine, but there are other hosted registries available for public use such as AWS and Google's own registries.

 a registry is a collection of repositories, while repositories hold a collection of related images.
************************************
Stateful Applications 
    - User session data is saved on the server side.
    - if the server goes down, it is difficult to transfer the session data to another server. 
    - This type of application will not work, if we want to implement autoscaling.
    - Stateful applications are suitable for scenarios where data persistence and strong consistency are vital, such as databases, financial applications, Auth Gateways, and ERP systems.


Stateless Applications
    - user session data is never saved on the server side.
    - using a common authentication gateway/client token method to validate the users once for multiple microservices.    

Monolithic and Microservice architecture

    Monolithic architecture
        - A monolithic application has a single code base with multiple modules in it.
        - It is a single build for the entire application.
        - To make minor changes to an application, we need to rebuild and re-deploy the complete application.
        - scaling is very challenging.

    Microservice architecture 
        - A microservice application is composed of small (micro) services. 
        - Each service will have a different code base.
        - Applications are divided into as small as possible sub-applications called services which are independent of each other and are called loosely coupled.    
        - Each service can be managed separately and it is deployable separately.
        - Services need not to share the same technology stack or frameworks.
*****************************************************************************************
main differences between AWS Network Access Control Lists (NACLs) and Security Groups: 

● Layer of defense: NACLs operate at the subnet level and control traffic in and out of a VPC, while Security Groups operate at the instance level and control traffic to and from individual EC2 instances. 
● Scope of application: NACLs apply to all instances in a subnet, while Security Groups apply to individual instances. 
● Statefulness: NACLs are stateless and do not track the state of a connection, while Security Groups are stateful and allow traffic based on the response to previous traffic. 
● Default rule: NACLs have a default rule that denies all traffic, while Security Groups have a default rule that allows all traffic. 
● Order of rules: NACLs have a numbered list of rules that are applied in order, while Security Groups do not have an order of rules. 
● Ability to block traffic: NACLs can block traffic at the subnet level, while Security Groups can only block traffic at the instance level. 
● Network performance: NACLs can potentially have a larger impact on network performance because they operate at the subnet level and apply to all instances in the subnet, while Security Groups only operate at the instance level and only apply to individual instances. 

*************************************************************************************************
A branching strategy, therefore, is the strategy that software development teams adopt when writing, merging and deploying code when using a version control system. It is essentially a set of rules that developers can follow to stipulate how they interact with a shared codebase.


What are Git tags?
Tags are ref's that point to specific points in Git history. Tagging is generally used to capture a point in history that is used for a marked version release (i.e. v1. 0.1). A tag is like a branch that doesn't change. Unlike branches, tags, after being created, have no further history of commits.

*****************************************************************************************
Labels and selectors
Labels are any key-value pairs that are used to identify that pod. The pod gets its label through the deployment which is like a blueprint for the pod before the pod is created. The Selector matches the label. Labels and selectors are required, to make connections between deployment, pods, and services.

Labels are the properties attached to each item/object.
Selector helps us to filter the items/objects which have labels attached to them.
**********************************************************************************
AWS services:
Amazon Web Services (AWS) offers a wide range of cloud computing services to meet various business needs. Here are some of the different types of AWS services:

Compute Services: Amazon Elastic Compute Cloud (EC2): Provides scalable virtual servers in the cloud.AWS Lambda: Enables serverless computing, allowing you to run code without provisioning or managing servers.AWS Batch: Manages and runs batch computing workloads on the cloud.
Storage and Content Delivery Services: Amazon Simple Storage Service (S3): Offers scalable object storage for data storage and retrieval.Amazon Elastic Block Store (EBS): Provides persistent block-level storage volumes for EC2 instances.Amazon Glacier: Provides long-term archival storage at a low cost.Amazon CloudFront: Delivers content globally with low latency through a content delivery network (CDN).
Database Services: Amazon Relational Database Service (RDS): Manages relational databases like MySQL, PostgreSQL, Oracle, and others.Amazon DynamoDB: Offers a fully managed NoSQL database service.Amazon Redshift: Provides a fast, fully managed data warehouse for analytics.Amazon Neptune: A fully managed graph database service.
Networking and Content Delivery Services: Amazon Virtual Private Cloud (VPC): Allows you to create isolated virtual networks in the AWS cloud.Amazon Route 53: Offers scalable domain name system (DNS) web services.AWS Direct Connect: Establishes a dedicated network connection between your premises and AWS.
Analytics Services: Amazon Athena: Allows you to query data in Amazon S3 using standard SQL statements.Amazon Kinesis: Collects, processes, and analyzes real-time streaming data.Amazon QuickSight: Provides business intelligence and visualization tools.Amazon EMR: Offers big data processing and analytics using Apache Hadoop, Spark, and other frameworks.
AI and Machine Learning Services: Amazon Rekognition: Provides image and video analysis with facial recognition and object detection.Amazon Polly: Converts text into lifelike speech.Amazon SageMaker: Offers a fully managed platform for building, training, and deploying machine learning models.Amazon Comprehend: Performs natural language processing to extract insights from text.
*************************************************
ps::The ‘ps’ command is a powerful tool for monitoring and managing processes in a Linux system. With examples provided in this article, you can easily use ‘ps’ command to view information about running processes, filter and sort output, and perform various other operations. By mastering ‘ps’ command, you can improve performance and stability of your Linux system, and troubleshoot problems more effectively.

In Linux, ps -ef is a command that displays information about running processes on the system. The ps command stands for “process status,” and the -ef option tells the command to display information about all processes in a full format.

The ps -ef command has numerous benefits, such as process identification, system monitoring, process management, security auditing, etc.
If you wish to display the processes that are running in the current shell, then you should execute the “ps” command without any parameters as follows: ps

Example 2: Display All the Currently Running Processes
You can also list down all the currently running processes of your Linux system with the following command:ps -A

Example 4: Display All the Processes Associated with a Particular User:  ps –u UserName
Example 5: Display All the Processes Associated with a Particular User Group ps –fG UserGroupName

The components of the ps -ef command are as follows:

ps: The command to display information about running processes on the system.
-e: An option to include all running processes in the output, including those not associated with the current terminal session.
-f: An option to display the output in a full format, which includes additional details such as process owner, parent process ID (PPID), CPU usage, and memory usage.

The output of the ps -ef command has several columns that are explained as follows:

UID: The user ID (UID) who started the process.
PID: The process ID (PID) of the running process.
PPID: The parent process ID (PPID) of the running process.
C: The CPU utilization of the process.
STIME: The start time of the process.
TTY: The controlling terminal associated with the process.
TIME: The CPU time used by the process.

o/p of particular process: ps -ef | grep <process name>
Display the PIDs Only: ps -ef | awk '{print $2}'
Display Process Sorted By a CPU Usage: ps -ef --sort=-%cpu
****************************************************************************************************
Blue green deployment:

A blue/green deployment is a deployment strategy in which you create two separate, but identical environments. One environment (blue) is running the current application version and one environment (green) is running the new application version. Using a blue/green deployment strategy increases application availability and reduces deployment risk by simplifying the rollback process if a deployment fails. Once testing has been completed on the green environment, live application traffic is directed to the green environment and the blue environment is deprecated.

A number of AWS deployment services support blue/green deployment strategies including Elastic Beanstalk, OpsWorks, CloudFormation, CodeDeploy, and Amazon ECS. Refer to Blue/Green Deployments on AWS for more details and strategies for implementing blue/green deployment processes for your application.
